# awesome-confidence-calibration

* [ICML 2017]  On Calibration of Modern Neural Networks [paper](https://arxiv.org/pdf/1706.04599.pdf)

* [ALMC 1997] Probabilistic outputs for support vector machines and comparison to regularized likelihood methods [paper](http://citeseer.ist.psu.edu/viewdoc/download;jsessionid=A375AE2C5B23A68414697D293F98B470?doi=10.1.1.41.1639&rep=rep1&type=pdf)

* [ICLR 2020] distance-based learning from errors for confidence calibration [paper](https://openreview.net/pdf?id=BJeB5hVtvB)

* [Arxiv 2019] Confidence Calibration for Convolutional Neural Networks Using Structured Dropout [paper](https://arxiv.org/pdf/1906.09551.pdf)

* [ICML 2016]  Dropout as a bayesian approximation: Representing model uncertainty in deep learning [paper](http://proceedings.mlr.press/v48/gal16.pdf) [code](https://github.com/yaringal/DropoutUncertaintyExps)

* [NIPS 2020] Improving model calibration with accuracy versus uncertainty optimization [paper](https://papers.nips.cc/paper/2020/file/d3d9446802a44259755d38e6d163e820-Paper.pdf) [code](https://github.com/IntelLabs/AVUC)

* [AISTATS 2011] Approximate inference for the loss-calibrated Bayesian [paper](http://proceedings.mlr.press/v15/lacoste_julien11a/lacoste_julien11a.pdf)

* [Arxiv 2018] Loss-calibrated approximate inference in bayesian neural networks [paper](https://arxiv.org/pdf/1805.03901.pdf)

* [CVPR 2019] Learning for Single-Shot Confidence Calibration in Deep Neural Networks through Stochastic Inferences [paper](https://openaccess.thecvf.com/content_CVPR_2019/papers/Seo_Learning_for_Single-Shot_Confidence_Calibration_in_Deep_Neural_Networks_Through_CVPR_2019_paper.pdf)

* [ICML 2005] Predicting Good Probabilities With Supervised Learning [apper](https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf)

* [KDD 2002] Transforming Classifier Scores into Accurate Multiclass Probability Estimates [paper](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.13.7457&rep=rep1&type=pdf)

* [NIPS 2017] On Fairness and Calibration [paper](https://arxiv.org/pdf/1709.02012v2.pdf)

* [Arxiv 2016] Approximating Likelihood Ratios with Calibrated Discriminative Classifiers [paper](https://arxiv.org/pdf/1506.02169v2.pdf)

* [NIPS 2020] Calibrating Deep Neural Networks using Focal Loss [paper](https://arxiv.org/pdf/2002.09437v2.pdf)

* [NIPS 2019] Verified Uncertainty Calibration [paper](https://arxiv.org/pdf/1909.10155v2.pdf) [code](https://github.com/p-lambda/verified_calibration)

* [CVPR 2021] Improving Calibration for Long-Tailed Recognition [paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhong_Improving_Calibration_for_Long-Tailed_Recognition_CVPR_2021_paper.pdf) [code](https://github.com/dvlab-research/MiSLAS)

* [CVPR 2021] Post-hoc Uncertainty Calibration for Domain Drift Scenarios [paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Tomani_Post-Hoc_Uncertainty_Calibration_for_Domain_Drift_Scenarios_CVPR_2021_paper.pdf) [code](https://github.com/tochris/calibration-domain-drift)

* [NIPS 2017] Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles [apper](https://proceedings.neurips.cc/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf)

* [NIPS 2019] Addressing Failure Prediction by Learning Model Confidence [paper](https://proceedings.neurips.cc/paper/2019/file/757f843a169cc678064d9530d12a1881-Paper.pdf) [code](https://github.com/valeoai/ConfidNet)

* [ICML 2018] Accurate Uncertainties for Deep Learning Using Calibrated Regression [paper](https://proceedings.mlr.press/v80/kuleshov18a/kuleshov18a.pdf)

* [ICLR 2018] Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples
 [paper](https://openreview.net/pdf?id=ryiAv2xAZ) [code](https://github.com/alinlab/Confident_classifier)

* [NIPS 2019] Addressing Failure Prediction by Learning Model Confidence [paper](https://proceedings.neurips.cc/paper/2019/file/757f843a169cc678064d9530d12a1881-Paper.pdf) [code](https://github.com/valeoai/ConfidNet)

* [Arxiv 2021] On the Calibration and Uncertainty of Neural Learning to Rank Models [paper](https://arxiv.org/pdf/2101.04356v1.pdf) [code](https://github.com/Guzpenha/transformer_rankers/tree/uncertainty_estimation)

* [Classic] Isotonic Regression [paper](https://www.stat.cmu.edu/~ryantibs/papers/neariso.pdf) [paper](https://arxiv.org/pdf/1603.04190.pdf) [paper](https://edoc.ub.uni-muenchen.de/966/1/Salanti_Georgia.pdf)

* [NIPS 2019] Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with Dirichlet calibration [paper](https://proceedings.neurips.cc/paper/2019/file/8ca01ea920679a0fe3728441494041b9-Paper.pdf)

* [ICML 2001] Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers [paper](https://cseweb.ucsd.edu/~elkan/calibrated.pdf)

* [AISTATS 2017] Beyond sigmoids: How to obtain well-calibrated probabilities from binary classifiers with beta calibration [paper](http://proceedings.mlr.press/v54/kull17a/kull17a.pdf) [code](https://github.com/REFRAME/betacal)

* [Arxiv 2015] Binary classifier calibration using an ensemble of near isotonic regression models. [paper](https://arxiv.org/pdf/1511.05191.pdf)

* [KDD 2019] Non-parametric Bayesian Isotonic Calibration: Fighting Over-Confidence in Binary Classification [paper](https://ecmlpkdd2019.org/downloads/paper/587.pdf) [code](https://github.com/mlkruup/bayesiso)

* [AAAI 2015] Obtaining Well Calibrated Probabilities Using Bayesian Binning [paper](https://people.cs.pitt.edu/~milos/research/AAAI_Calibration.pdf) [code](https://github.com/pakdaman/calibration)

* [Arxiv 2021] Distribution-free calibration guarantees for histogram binning without sample splitting [paper](https://arxiv.org/pdf/2105.04656.pdf) [code](https://github.com/aigen/df-posthoc-calibration)

* [ICML 2012] Predicting accurate probabilities with a ranking loss [paper](https://icml.cc/2012/papers/372.pdf)

## Resources

* Statistical Decision Theory and Bayesian Analysis [book](https://link.springer.com/book/10.1007/978-1-4757-4286-2)

* A Tutorial on Learning With Bayesian Networks [paper](https://arxiv.org/pdf/2002.00269.pdf)

## Application

* [AAAI 2021] Learning to Cascade: Confidence Calibration for Improving the Accuracy and Computational Cost of Cascade Inference Systems [paper](https://ojs.aaai.org/index.php/AAAI/article/download/16900/16707)
